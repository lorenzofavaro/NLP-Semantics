{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this exercise we need to find the correct synset starting from the definitions given by us (unito students) for 4 terms. To direct our search we will use the principle of \"genus\".\n",
    "\n",
    "#### The approach\n",
    " 1. Find the genus candidates (typically the most frequent terms in the definitions)\n",
    " 2. Collect the wordnet synsets for:\n",
    "    - each genus candidate\n",
    "    - each hyponym of each genus candidate\n",
    "    - each hypernym of each genus candidate\n",
    " 3. Get the wordnet signature (definition + examples) of each collected synset\n",
    " 4. Compare wordnet signature and the definitions through Lesk (overlap)\n",
    " 5. Choose the synset that has the highest score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from typing import Set, Dict, AnyStr, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '..'\n",
    "\n",
    "# Load stop words from file\n",
    "def import_stop_words() -> Set[AnyStr]:\n",
    "    with open(f'{root_dir}/common-data/stop_words_FULL.txt') as f:\n",
    "        stop_words = {line for line in f.read().splitlines()}\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "# Load definitions and build a map\n",
    "def import_definitions_map() -> Dict[AnyStr, List[Set[AnyStr]]]:\n",
    "    definitions_map = defaultdict(list)\n",
    "    with open(f'{root_dir}/common-data/definitions.csv') as f:\n",
    "\n",
    "        for line in f.readlines()[1:]:\n",
    "            splits = line.split(\"~|~\")\n",
    "            word = splits[0]\n",
    "            definitions = splits[1:]\n",
    "            \n",
    "            for definition in definitions:\n",
    "                if bow_words := bag_of_words(definition):\n",
    "                    definitions_map[word].append(bow_words)\n",
    "\n",
    "    return dict(definitions_map)\n",
    "            \n",
    "\n",
    "stop_words = import_stop_words()\n",
    "definitions_map = import_definitions_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing of a sentence\n",
    "def bag_of_words(sentence: AnyStr) -> Set[AnyStr]:\n",
    "    return set(remove_stopwords(tokenize_sentence(remove_punctuation(sentence))))\n",
    "\n",
    "\n",
    "# Remove stopwords from a word list\n",
    "def remove_stopwords(words: List[AnyStr]) -> List[AnyStr]:\n",
    "    return [value.lower() for value in words if value.lower() not in stop_words]\n",
    "\n",
    "\n",
    "# Get tokens from sentence\n",
    "def tokenize_sentence(sentence: AnyStr) -> List[AnyStr]:\n",
    "    words = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        words.append(lmtzr.lemmatize(tag[0]).lower())\n",
    "    return words\n",
    "\n",
    "\n",
    "# Remove punctuation and multiple spaces\n",
    "def remove_punctuation(sentence: AnyStr) -> AnyStr:\n",
    "    return re.sub('\\s\\s+', ' ', re.sub(r'[^\\w\\s]', '', sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the |words_num| most frequent words present in the definitions\n",
    "def find_MFW(definitions: List[Set[AnyStr]], words_num: int) -> List[AnyStr]:\n",
    "    concept_counter = Counter()\n",
    "    \n",
    "    for definition in definitions:\n",
    "        concept_counter.update(definition)\n",
    "    \n",
    "    most_frequent_words = [entry[0] for entry in concept_counter.most_common(words_num)]\n",
    "    return most_frequent_words\n",
    "\n",
    "# Return first wordnet synset for a word\n",
    "def get_synset(word: AnyStr) -> Synset: # maybe it's possible to do disambiguation using the words in the sentence (currently just choose the 1st synset)\n",
    "    synset = None\n",
    "    if synsets := wordnet.synsets(word):\n",
    "        synset = synsets[0]\n",
    "\n",
    "    return synset\n",
    "\n",
    "# Compute the most likely synset from just the definitions, using |freq_words_num| most frequent words\n",
    "def compute_results(freq_words_num: int) -> Dict[AnyStr, Dict[AnyStr, List[AnyStr]]]:\n",
    "    results = defaultdict(dict)\n",
    "    for concept, definitions in definitions_map.items():\n",
    "        most_frequent_words = find_MFW(definitions, freq_words_num)\n",
    "\n",
    "        hyponyms = []\n",
    "        for word in most_frequent_words:\n",
    "            if synset := get_synset(word):\n",
    "                hyponyms.extend(synset.hyponyms())\n",
    "        \n",
    "        res = []\n",
    "        for hyp in hyponyms:\n",
    "            hyp_def = hyp.definition() + ','.join(hyp.examples())\n",
    "            \n",
    "            match_words = []\n",
    "            for word in most_frequent_words:\n",
    "                if word in hyp_def:\n",
    "                    match_words.append(word) # score = |words found|\n",
    "            \n",
    "            res.append([hyp, match_words])\n",
    "    \n",
    "        # sort the list using the number of important words found\n",
    "        sorted_res = sorted(res, key=lambda x: len(x[1]), reverse=True)\n",
    "        for synset, match_words in sorted_res[:freq_words_num]:\n",
    "            results[concept][synset.name()] = match_words\n",
    "\n",
    "    return dict(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION\n",
      "affection.n.01: ['feeling', 'feel', 'express']\n",
      "pain.n.02: ['feeling', 'feel', 'mental']\n",
      "pleasure.n.01: ['feeling', 'feel', 'mental']\n",
      "affect.n.01: ['feeling', 'feel']\n",
      "agitation.n.03: ['feeling', 'feel']\n",
      "ambivalence.n.01: ['feeling', 'feel']\n",
      "astonishment.n.01: ['feeling', 'feel']\n",
      "calmness.n.03: ['feeling', 'feel']\n",
      "desire.n.01: ['feeling', 'feel']\n",
      "despair.n.02: ['feeling', 'feel']\n",
      "\n",
      "PERSON\n",
      "homo_habilis.n.01: ['human', 'characteristic']\n",
      "world.n.08: ['human', 'living']\n",
      "beard.n.03: ['person', 'homo']\n",
      "homosexual.n.01: ['person', 'homo']\n",
      "life.n.08: ['person', 'living']\n",
      "man.n.03: ['human', 'generic']\n",
      "man_jack.n.01: ['individual', 'single']\n",
      "self.n.02: ['person', 'individual']\n",
      "\n",
      "REVENGE\n",
      "hate.n.01: ['feeling', 'action', 'emotion']\n",
      "lightning_rod.n.01: ['action', 'reaction', 'negative']\n",
      "sounding_board.n.01: ['action', 'reaction', 'person']\n",
      "dander.n.02: ['anger', 'feeling']\n",
      "fury.n.01: ['anger', 'feeling']\n",
      "indignation.n.01: ['anger', 'feeling']\n",
      "infuriation.n.01: ['anger', 'feeling']\n",
      "umbrage.n.01: ['anger', 'feeling']\n",
      "affect.n.01: ['feeling', 'emotion']\n",
      "ambivalence.n.01: ['feeling', 'emotion']\n",
      "\n",
      "BRICK\n",
      "library.n.05: ['material', 'build', 'building', 'house']\n",
      "clubhouse.n.01: ['build', 'building', 'house']\n",
      "government_building.n.01: ['build', 'building', 'house']\n",
      "hotel-casino.n.02: ['build', 'building', 'house']\n",
      "house.n.12: ['build', 'building', 'house']\n",
      "packinghouse.n.01: ['build', 'building', 'house']\n",
      "telecom_hotel.n.01: ['build', 'building', 'house']\n",
      "theater.n.01: ['build', 'building', 'house']\n",
      "dry_walling.n.01: ['build', 'building']\n",
      "erecting.n.01: ['build', 'building']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq_words_num = 7\n",
    "results = compute_results(freq_words_num=freq_words_num)\n",
    "\n",
    "# print results\n",
    "for concept, items in results.items():\n",
    "    print(concept.upper())\n",
    "    for item, match_words in items.items():\n",
    "        print(f\"{item}: {match_words}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c75d46810c3de735ca95efd8dfb7d6dbf93a2c06ee0538e38e90ee10bc420c21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
