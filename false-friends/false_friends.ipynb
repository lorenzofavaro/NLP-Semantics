{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this exercise we see the definition of a false friends word detection algorithm. The general definition of a false friend is that of two almost homonymous words that share many characters in common but which differ greatly in meaning.\n",
    "We used the lexical resource WordNet to access the different meanings (synsets) of the terms.\n",
    "\n",
    "To understand whether or not two words are lexically similar, we decided to look at their **Edit Distance**, that is the minimum number of operations of insertion, removal, modification, to transform one string into another.\n",
    "\n",
    "After that, to check if two words are False Friends we checked their **Wu & Palmer similarity** making sure it is less than a certain threshold.\n",
    "\n",
    "This way, terms with **high lexical similarity** and **low semantic similarity** are good candidates to be False Friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed, randint\n",
    "from nltk import edit_distance\n",
    "from nltk.corpus import semcor, wordnet\n",
    "from nltk import Tree\n",
    "from itertools import combinations\n",
    "from typing import List, Union, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemCor tagged corpus\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute |sentence_num| random sentences from the SemCor corpus\n",
    "def compute_random_sentences(sentence_num: int, custom_seed: int=None) -> List[str]:\n",
    "    seed(custom_seed)\n",
    "    max_index = 10000\n",
    "    \n",
    "    indices = set()\n",
    "    while len(indices) != sentence_num:\n",
    "        index = randint(0, max_index)\n",
    "        indices.add(index)\n",
    "    \n",
    "    sentences = [tagged_sentences[index] for index in indices]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Get content words from the random sentences got before\n",
    "def get_content_words(tagged_sentences: List[List[Union[str, Tree]]]) -> List[str]:\n",
    "    content_words = []\n",
    "    for sentence in tagged_sentences:\n",
    "        for word in sentence:\n",
    "            if type(word) is Tree and type(word.label()) != str and word.label().synset().pos() in [\"n\", \"v\", \"s\", \"r\"] and len(word[0]) > 3:\n",
    "                try:\n",
    "                    content_words.append(word[0].lower())\n",
    "                except:\n",
    "                    content_words.extend([el.lower() for el in word[0]]) # The element is a multi-word expression\n",
    "\n",
    "    \n",
    "    return content_words\n",
    "\n",
    "\n",
    "# Compute pairs that are lexically similar, it uses the edit distance to measure it (edit distance < threshold)\n",
    "def compute_close_pairs(content_words: List[str], threshold: int=2) -> List[Tuple[str, str]]:\n",
    "    pairs = set(combinations(content_words, 2))\n",
    "    close_pairs = [pair for pair in pairs if edit_distance(pair[0], pair[1]) < threshold] # Only keeps words lexically close\n",
    "    close_pairs = [pair for pair in close_pairs if edit_distance(pair[0], pair[1]) != 0] # Delete pairs of identical words\n",
    "    return close_pairs\n",
    "\n",
    "\n",
    "# Compute False Friends, i.e. words that are lexically similar but semantically different. It uses the Wu & Palmer similarity to measure it\n",
    "def compute_false_friends(close_pairs: List[Tuple[str, str]], threshold: float) -> List[Tuple[str, str]]:\n",
    "    false_friends = []\n",
    "    for pair in close_pairs:\n",
    "        synsets_1 = wordnet.synsets(pair[0])\n",
    "        synsets_2 = wordnet.synsets(pair[1])\n",
    "        if not (synsets_1 and synsets_2):\n",
    "            continue\n",
    "\n",
    "        similarity = wordnet.wup_similarity(synsets_1[0], synsets_2[0]) # wu&palmer similarity between the first wordnet synsets\n",
    "        if similarity < threshold:\n",
    "            false_friends.append(pair)\n",
    "    \n",
    "    return false_friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Friends Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lost', 'most'), ('fact', 'face')]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "sentence_num = 20\n",
    "edit_threshold = 2\n",
    "similarity_threshold = 0.3\n",
    "\n",
    "sentences = compute_random_sentences(sentence_num)\n",
    "content_words = get_content_words(sentences)\n",
    "close_pairs = compute_close_pairs(content_words, threshold=edit_threshold)\n",
    "false_friends = compute_false_friends(close_pairs, threshold=similarity_threshold)\n",
    "\n",
    "print(false_friends)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea59138689929fdab3e49db7a63d86373d5c14a34dc5888abf301d9cce48d630"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
