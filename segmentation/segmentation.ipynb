{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do\n",
    "Implement a simple algorithm on **Text Segmentation**:\n",
    "- Use as a test an input of k paragraphs taken from different topics (e.g. Wikipedia pages)\n",
    "- Is our system able to find the right cuts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from collections import Counter\n",
    "from random import randint\n",
    "from typing import Set, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_corpora():\n",
    "    with open('data/topics_corpora.txt', encoding='utf-8') as f:\n",
    "        sentences = f.read().splitlines()\n",
    "    return sentences\n",
    "\n",
    "corpora = import_corpora()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing of a sentence\n",
    "def bag_of_words(sentence: str) -> Set[str]:\n",
    "    return set(remove_stopwords(tokenize_sentence(remove_punctuation(sentence))))\n",
    "\n",
    "\n",
    "# Remove stopwords from a word list\n",
    "def remove_stopwords(words: List[str]) -> List[str]:\n",
    "    return [value.lower() for value in words if value.lower() not in stop_words]\n",
    "\n",
    "\n",
    "# Get tokens from sentence\n",
    "def tokenize_sentence(sentence: str) -> List[str]:\n",
    "    words = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        words.append(lmtzr.lemmatize(tag[0]).lower())\n",
    "    return words\n",
    "\n",
    "\n",
    "# Remove punctuation and multiple spaces\n",
    "def remove_punctuation(sentence: str) -> str:\n",
    "    return re.sub('\\s\\s+', ' ', re.sub(r'[^\\w\\s]', '', sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30 Most Common Words Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 49),\n",
       " ('football', 45),\n",
       " ('wa', 39),\n",
       " ('computer', 32),\n",
       " ('first', 27),\n",
       " ('image', 26),\n",
       " ('game', 26),\n",
       " ('music', 24),\n",
       " ('camera', 23),\n",
       " ('science', 22),\n",
       " ('color', 21),\n",
       " ('field', 20),\n",
       " ('used', 18),\n",
       " ('also', 17),\n",
       " ('one', 17),\n",
       " ('ball', 17),\n",
       " ('system', 16),\n",
       " ('software', 15),\n",
       " ('played', 15),\n",
       " ('movie', 15),\n",
       " ('digital', 14),\n",
       " ('known', 14),\n",
       " ('computation', 13),\n",
       " ('using', 13),\n",
       " ('many', 13),\n",
       " ('picture', 13),\n",
       " ('theory', 12),\n",
       " ('data', 12),\n",
       " ('process', 12),\n",
       " ('association', 12)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_MCW = 30 # num of most common words\n",
    "\n",
    "counter = Counter()\n",
    "for sentence in corpora:\n",
    "    counter.update(bag_of_words(sentence))\n",
    "\n",
    "counter.most_common(n_MCW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuts Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cuts(n_cuts, max_value):\n",
    "    cuts = [randint(0, max_value-1) for _ in range(n_cuts-1)]\n",
    "    return sorted(cuts)\n",
    "\n",
    "\n",
    "def co_occurrence(segment, n_MCW):\n",
    "    score = 0\n",
    "    counter = Counter()\n",
    "    for sentence in segment:\n",
    "        counter.update(bag_of_words(sentence))\n",
    "    \n",
    "    most_common_words = counter.most_common(n_MCW)\n",
    "    score = sum([entry[1] for entry in most_common_words])\n",
    "    return score\n",
    "\n",
    "\n",
    "def compute_score(cuts, corpora, n_MCW):\n",
    "    scores = []\n",
    "\n",
    "    for j in range(len(cuts) + 1):\n",
    "        start = cuts[j-1] if j > 0 else 0\n",
    "        end = cuts[j] if j < len(cuts) else len(corpora) - 1\n",
    "        segment = corpora[start:end]\n",
    "        \n",
    "        score = co_occurrence(segment, n_MCW)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return sum(scores)\n",
    "    \n",
    "\n",
    "def maximize_score(corpora, n_MCW, n_topics, max_iterations, adjustment_range = 10):\n",
    "    max_score = 0\n",
    "    best_cuts = None\n",
    "    for i in range(max_iterations):\n",
    "        # Compute random cuts\n",
    "        cuts = random_cuts(n_topics, len(corpora)-1)\n",
    "        # Compute score for the cuts obtained\n",
    "        current_score = compute_score(cuts, corpora, n_MCW)\n",
    "        if current_score > max_score:\n",
    "            max_score = current_score\n",
    "            best_cuts = cuts\n",
    "            print(f\"completed iteration {i+1} - max_score: {max_score}, best_cuts: {best_cuts}\")\n",
    "    \n",
    "    cuts = best_cuts.copy()\n",
    "    for i in range(len(cuts)):\n",
    "        for adjustment in range(-adjustment_range//2, adjustment_range//2):\n",
    "            cuts[i] += adjustment\n",
    "            current_score = compute_score(cuts, corpora, n_MCW)\n",
    "            if current_score > max_score:\n",
    "                max_score = current_score\n",
    "                best_cuts = cuts\n",
    "                print(f\"refining cuts - max_score: {max_score}, best_cuts: {best_cuts}\")\n",
    "\n",
    "    return max_score, best_cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed iteration 1 - max_score: 243, best_cuts: [60, 83, 137]\n",
      "completed iteration 10 - max_score: 257, best_cuts: [50, 145, 230]\n",
      "completed iteration 22 - max_score: 260, best_cuts: [75, 157, 236]\n",
      "completed iteration 34 - max_score: 270, best_cuts: [71, 157, 238]\n",
      "completed iteration 90 - max_score: 278, best_cuts: [63, 153, 255]\n",
      "Max score is: 278\n",
      "Best cuts indices are: [63, 153, 255]\n"
     ]
    }
   ],
   "source": [
    "n_MCW = 3 # num of most common words\n",
    "n_topics = 4\n",
    "max_iterations = 1000\n",
    "adjustment_range = 5\n",
    "\n",
    "max_score, best_cuts = maximize_score(corpora, n_MCW, n_topics, max_iterations, adjustment_range=adjustment_range)\n",
    "print(f'Max score is: {max_score}')\n",
    "print(f'Best cuts indices are: {best_cuts}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c75d46810c3de735ca95efd8dfb7d6dbf93a2c06ee0538e38e90ee10bc420c21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
